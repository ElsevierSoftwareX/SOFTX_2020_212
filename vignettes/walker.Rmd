---
title: "Efficient Bayesian linear regression with time-varying coefficients"
author: "Jouni Helske"
date: "8 June 2017"
output:
html_document: default
bibliography: walker.bib
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(walker)
```

# Introduction

Dynamic linear regression models are extension to basic linear regression models where instead of constant but unknown regression coefficients, the underlying coefficients are assumed to vary over "time" according to random walk. These types of models allow robust modelling of phenomenas where the effect size of the predictor variables and the response variable can vary during the period of the study. The `R` [@R] package `walker` provides an efficient method for fully Bayesian inference of such models, where the main computations are performed using state-of-the-art Markov chain Monte Carlo (MCMC) algorithms provided by `Stan` [@stan, @rstan]. This also allows the straightforward use of many diagnostic and graphical tools provided by several `Stan` related `R` packages such as `ShinyStan` [@shinystan].

More specifically, the dynamic regression model is defined as
$$
\begin{aligned}
y_t &= x_t \beta_t + \epsilon_t, \quad t = 1,\ldots, n\\
\beta_{t+1} &= \beta_t + \eta_t,
\end{aligned}
$$
where $y_t$ is the observation at time $t$, $x_t$ contains the corresponding predictor variables, $\beta_t$ is a $k$ dimensional vector of regression coefficients at time $t$, $\epsilon_t \sim N(0, \sigma^2_{\epsilon})$, and $\eta_t \sim N(0, D)$, with $D$ being $k \times k$ diagonal matrix with diagonal elements $\sigma^2_{i,\eta}$, $i=1,\ldots,k$. Denote the unknown parameters of the model by $\beta = (\beta_1, \ldots, \beta_n)$ and $\sigma = (\sigma_{\epsilon}, \sigma_{1, \eta}, \ldots, \sigma_{k, \eta})$. We define priors for first $\beta_1$ as $N(\mu_{\beta_1}, \sigma_{\beta_1})$, and for $\sigma_i \sim N(\mu_{\sigma_i}, \sigma_{\sigma_i})$, $i=1,\ldots,k+1$, truncated to positive values, with slighly awful notation.

Although in principle writing dynamic regression model above in `Stan` language is straighforward, most intuitive implementations are computationally inefficient and prone to severe problems related to convergence of the underlying MCMC algorithm. The approach used by `walker` is based on the marginalization of the regression coefficients $\beta$ during the MCMC sampling by using the Kalman filter, and which provides fast and accurate inference of marginal posterior $p(\sigma | y)$, and the corresponding joint posterior $p(\sigma, \beta | y) = p(\beta | \sigma, y)p(\sigma | y)$ can then be obtained by simulating the regression coefficients given sampled standard deviations using the Kalman smoothing based simulation algorithms such as [@durbin-koopman2002]. Note that we have opted to sample the $\beta$ parameters given $\sigma$'s, but it is also possible to obtain somewhat more accurate summary statistics such as mean and variance of these parameters by using the standard Kalman smoother for compution of $\textrm{E}(\beta| \sigma, y)$ and $\textrm{Var}(\beta| \sigma, y)$, and using the law of total expectation.

## Illustration

Let us consider a model with two predictors and observations of length $n=100$. This is rather small problem, but it was chosen in order to make possible comparisons with the "naive" implementation. For larger problems (in terms of number of observations and especially number of predictors) it is very difficult to get naive implementation to work at all, as even after tweaking several parameters of the underlying MCMC sampler, one typically ends up with divergent transitions or low BMFI index, meaning that the results are not to be trusted. 

First we simulate the coeffients and the predictors:

```{r example}
set.seed(123)
n <- 100
beta1 <- cumsum(c(0.5, rnorm(n - 1, 0, sd = 0.05)))
beta2 <- cumsum(c(-0.1, rnorm(n - 1, 0, sd = 0.15)))
x1 <- rnorm(n, 1)
x2 <- cos(1:n)
u <- cumsum(rnorm(n))
ts.plot(cbind(u, beta1 * x1, beta2 * x2), col = 1:3)
```

```{r observations}
signal <- u + beta1 * x1 + beta2 * x2
y <- rnorm(n, signal)
ts.plot(y)
lines(signal, col = 2)
```

Then we can call function `walker`. The model is defined as a formula like in `lm`, and we can give several arguments which are passed to `sampling` method of `rstan`, such as number of iteration `iter` and number of chains `chains`. In addition to these, we use arguments `beta_prior` and `sigma_prior`, which define the prior distributions for $\beta$ and $\sigma$ respectively. These arguments should be two-column matrices, where the first column defines the prior means, and the second column defines the prior standard deviations.

```{r walker}
kalman_walker <- walker(y ~ x1 + x2, iter = 2000, chains = 1, seed = 1, refresh = 0,
  beta_prior = cbind(0, rep(2, 3)), sigma_prior = cbind(0, rep(2, 4)))
print(kalman_walker, pars = "sigma")
plot(kalman_walker, pars = "sigma")
```

We often get few (typically one) warning message about numerical problems, as the sampling algorithm warms up, but this is nothing to be concerned with (if more errors occur, then a Github issue for `walker` package is more than welcome). 

Using the `extract` method of `rstan` we can pick up the samples corresponding to $\beta$'s, and we can for example plot the posterior mean paths with `ts.plot` (the dashed lines correspond to true coefficients):

```{r plot_betas}
betas <- extract(kalman_walker, "beta")[[1]]
ts.plot(cbind(u, beta1, beta2, apply(betas, 2, colMeans)),
  col = 1:3, lty = rep(2:1, each = 3))
```

We can perform the same analysis with naive implementation by setting the argument `naive` to `TRUE`:
```{r naive}
naive_walker <- walker(y ~ x1 + x2, iter = 2000, chains = 1, seed = 1, refresh = 0,
  beta_prior = cbind(0, rep(2, 3)), sigma_prior = cbind(0, rep(2, 4)),
  naive = TRUE)
print(naive_walker, pars = c("sigma"))

# we need to do this "manually" as it currently does not work automatically for single chain
rstan:::throw_sampler_warnings(naive_walker) 
```

With naive implementation we get smaller effective sample sizes and higher computation time, as well as some indications of divergence problems.

# Discussion

In this vignette we illustrated the benefits of marginalisation in dynamic regression context. The underlying idea is not new; this approach is typical especially in classic Metropolis-type algorithms for linear-Gaussian state space models where the marginal likelihood $p(y | \theta)$ is used in the computation of the acceptance probability. Here instead of building specific MCMC machinery, we rely on readily available Hamiltonian Monte Carlo based `Stan` software, thus allowing us to enjoy the benefits of diverse tools of the `Stan` community. Due to the restricted class of models considered, we can also simplify the underlying Kalman filter considerably, thus leading efficient likelihood evaluation.

## References
